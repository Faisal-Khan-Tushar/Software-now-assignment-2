{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OabeYw93sTr",
        "outputId": "7fdaeb9f-6c44-4fcd-f63a-3fe8662a30ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*: 12214251\n",
            ".: 8947097\n",
            "-: 6203884\n",
            ":: 4112220\n",
            ",: 3666144\n",
            "[: 2778940\n",
            "]: 2778846\n",
            "): 2241033\n",
            "(: 2194265\n",
            "the: 2135417\n",
            "and: 1873788\n",
            "to: 1620980\n",
            "of: 1572498\n",
            "was: 1471942\n",
            "1: 1325148\n",
            "with: 1123084\n",
            "/: 1083591\n",
            "2: 1066626\n",
            "a: 1061508\n",
            "in: 1045065\n",
            "on: 1008914\n",
            "name: 860545\n",
            "for: 817392\n",
            "3: 741510\n",
            "no: 707852\n",
            "##s: 707329\n",
            "5: 701618\n",
            "mg: 665025\n",
            "##g: 649896\n",
            "4: 644311\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "def count_unique_tokens(text_file_path):\n",
        "    \"\"\"Counts unique tokens in a text file using the AutoTokenizer function from the Transformers library.\n",
        "\n",
        "    Args:\n",
        "        text_file_path: The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains a token and its count. The list is sorted in descending order by count.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the AutoTokenizer\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Initialize an empty dictionary to store token counts\n",
        "    token_counts = {}\n",
        "\n",
        "    # Open the file and process it line by line to reduce memory usage\n",
        "    with open(text_file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            # Tokenize the current line\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "\n",
        "            # Update the token counts\n",
        "            for token in tokens:\n",
        "                token_counts[token] = token_counts.get(token, 0) + 1\n",
        "\n",
        "    # Sort the token counts by frequency in descending order\n",
        "    sorted_token_counts = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_token_counts[:30]\n",
        "\n",
        "# Example usage\n",
        "text_file_path = \"/content/all_combined_texts.txt\"\n",
        "top_30_tokens = count_unique_tokens(text_file_path)\n",
        "\n",
        "# Print the top 30 tokens\n",
        "for token, count in top_30_tokens:\n",
        "    print(f\"{token}: {count}\")\n"
      ]
    }
  ]
}